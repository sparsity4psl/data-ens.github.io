---
layout: page
title: "Program"
description: "<b>Sparsity for Physics, Signal and Learning <br> June 24th - 27th 2019</b>"
header-img: "img/inria.png"
---

The given titles are preliminary, the definitive program is coming soon.

## Courses
* [Yohann De Castro](https://ydecastro.github.io/) (ENPC)  
**Measures estimation with moments, off-the-grid inverse problems**

* [Emilie Chouzenoux](http://www-syscom.univ-mlv.fr/~chouzeno/) (Paris Est Marne-la-Vallée)  
**Optimisation and applications in imaging**

* [Karen Veroy-Grepl](https://www.aices.rwth-aachen.de/en/about-aices/people/principal-investigators/details-zur-person/veroy-grepl) (RWTH-Aachen)  
**Introduction to Reduced Basis Methods: Theory and Applications**

* [Bernard Haasdonk](https://www.ians.uni-stuttgart.de/institute/team/Haasdonk-00005/) (Stuttgart University)  
**Kernel Methods for Surrogate Modelling**  
 *Abstract*: Kernel methods are a very efficient class of algorithms
 in numerical analysis and machine learning.
 For example they allow function interpolation or approximation,
 PDE solution by collocation, data analysis by
 classification, regression and novelty detection.
 From a theoretical viewpoint they have a functional analytical
 background in Reproducing Kernel Hilbert Spaces (RKHS), that allows
 error and convergence analysis.
 Common goal for efficiency is sparsity in the kernel expansions.
 Apart from well known sparsity-inducing optimization targets, also
 greedy techniques are of high interest.
 We will present the theory and application of such methods,
 with particular focus on surrogate modelling.

## Talks
* [Albert Cohen](https://www.ljll.math.upmc.fr/cohen/) (Sorbonne Universités)  
**High-dimensional sparse approximation**

* [Yvon Maday](https://www.ljll.math.upmc.fr/maday/) (Sorbonne Universités)  
**Mixing data and reduced models for rapid predictions and control**
 *Abstract*: Reconstruction of unknown functions from data has become one of the hottest problems in sciences and industry, especially that plethora of data are now available and people would like to use this amount to get more knowledge. Recent advances in AI (artificial intelligence) provided e.g. by deep neural network incite to get rid on profound understanding of the phenomenon and rely on the neural network to find the information by it self. If this may be true for many problems, in most situations the data are polluted with errors, are not structured enough or are incomplete. In these numerous cases, there is still need to HI (human intelligence) to provide the backbone of the data assimilation and/or treatment and help in getting stability and robustness. We shall present in this talk various applications of the use of reduced basis methods as such a backbone in case of different sizes of available data, and also of different noise level in these data.

* [Laure Blanc-Féraud](http://www-sop.inria.fr/members/Laure.Blanc_Feraud/) (CNRS I3S)  
**Sparse norm relaxation and applications in microscopy**

* [Rémi Gribonval](https://people.irisa.fr/Remi.Gribonval/) (INRIA Rennes)  
**Compressed Learning**
